{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPump\n",
    "\n",
    "### This is a project whose purpose is to construct a generative network to generate new song lyrics. The network will train on lyrics from lyricsgenius -- a website hosting plaintext lyrics from just abotu every popular song ever released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We start by accessing the lyricsgenius API to download lyrics for a given artist (for demonstration we use lil' Pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the lyricsgenius python package (can be installed with pip install lyricsgenius)\n",
    "import lyricsgenius\n",
    "#import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import numpy.random as random\n",
    "\n",
    "import keras.models as km\n",
    "import keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input the client token (acquired from making a lyricsgenius account)\n",
    "client_token = 's86ExsILIhrEXrTfQmePOYXJ6jPT9KACHQ22dXs960suWEpwa4HQwUn56AB5Gsx7'\n",
    "\n",
    "# Create an instance of lyrics genius\n",
    "genius = lyricsgenius.Genius(client_token)\n",
    "\n",
    "# Pick an artist and save their lyrics, here we use Lil Pump, and we choose to sort by title\n",
    "# NOTE -- THE LYRICS GENIUS API WILL SOMETIMES TIME OUT -- THIS IS AN ISSUE WITH HE API. JUST RERUN UNTIL IT WORKS.\n",
    "artist_name = \"Lil Pump\"\n",
    "artist_tag=artist_name.replace(\" \",\"\")\n",
    "\n",
    "# Check if we have already downloaded this data, skip download if you have\n",
    "if glob.glob(\"Lyrics_\"+artist_tag+\".json\")==[]:\n",
    "    artist = genius.search_artist(artist_name, sort=\"title\")\n",
    "    artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the song lyric names\n",
    "lyric_files = glob.glob(\"Lyrics_\"+artist_tag+\".json\")\n",
    "\n",
    "# load the lyrics into a dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(lyric_files)):\n",
    "    predf = pd.read_json(lyric_files[i],orient='index',typ='series')\n",
    "    df = df.append(predf.songs)\n",
    "    \n",
    "# Store title and lyrics in a dataframe\n",
    "data = df[['title','lyrics']]\n",
    "data.sample(3)\n",
    "\n",
    "data.to_csv('lyrics_titles_'+artist_tag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('lyrics_titles_'+artist_tag+'.csv')\n",
    "# Create a string of the entire set of lyrics\n",
    "corpus0 = \"\"\n",
    "for row in data.itertuples():\n",
    "    text = row.lyrics\n",
    "    # Append all the text from the lyrics to \"all_lyrics\"\n",
    "    if type(text) == str:\n",
    "        corpus0+=text\n",
    "\n",
    "# Remove unwanted characters/strings from the corpus\n",
    "corpus0 = corpus0.replace('[verse]' ,'')\n",
    "corpus0 = corpus0.replace('[intro]', '')\n",
    "corpus0 = corpus0.replace('[outro]', '')\n",
    "corpus0 = corpus0.replace('[bridge]', '')\n",
    "corpus0 = corpus0.replace('[chorus]' ,'')\n",
    "corpus0 = corpus0.replace('[Intro]', '')\n",
    "corpus0 = corpus0.replace('[Outro]', '')\n",
    "corpus0 = corpus0.replace('[Bridge]', '')\n",
    "corpus0 = corpus0.replace('[Chorus]', '')\n",
    "corpus0 = corpus0.replace('[verse 1]', '')\n",
    "corpus0 = corpus0.replace('[verse 2]', '')\n",
    "corpus0 = corpus0.replace('[verse 3]', '')\n",
    "corpus0 = corpus0.replace('[verse 4]', '')\n",
    "corpus0 = corpus0.replace('Lyrics', '')\n",
    "\n",
    "# Include spaces between punctuation and words, to reduce unique words\n",
    "corpus0 = corpus0.replace(',', ' , ')\n",
    "corpus0 = corpus0.replace('(', ' , ')\n",
    "corpus0 = corpus0.replace(')', ' ) ')\n",
    "corpus0 = corpus0.replace('[', ' [ ')\n",
    "corpus0 = corpus0.replace(']', ' ] ')\n",
    "corpus0 = corpus0.replace('.', ' . ')\n",
    "corpus0 = corpus0.replace(';', ' ; ')\n",
    "corpus0 = corpus0.replace(':', ' : ')\n",
    "corpus0 = corpus0.replace('!', ' ! ')\n",
    "corpus0 = corpus0.replace('?', ' ? ')\n",
    "corpus0 = corpus0.replace('*', ' * ')\n",
    "corpus0 = corpus0.replace(\"’\", '\\'')\n",
    "corpus0 = corpus0.replace(\"\\'\\'\", ' \" ')\n",
    "corpus0 = corpus0.replace('\"', ' \" ')\n",
    "corpus0 = corpus0.replace(\"'\", \" ' \")\n",
    "corpus0 = corpus0.replace('\\r\\n', ' \\r\\n ')\n",
    "corpus0 = corpus0.replace('-', ' - ')\n",
    "corpus0 = corpus0.replace('\\n', ' \\n ')\n",
    "corpus0 = corpus0.replace('\\u2005', ' ')\n",
    "corpus0 = corpus0.replace('\\u205f', ' ')\n",
    "corpus0 = corpus0.replace('—', ' — ')\n",
    "corpus0 = corpus0.replace('¿', ' ¿ ')\n",
    "corpus0 = corpus0.replace('¡', ' ¡ ')\n",
    "\n",
    "  \n",
    "\n",
    "# Convert the text to lower case so that lower and uppercase are not treated differently\n",
    "corpus0 = corpus0.lower()\n",
    "\n",
    "# Split the words by spaces; \n",
    "corpus1 = corpus0.split(' ')\n",
    "\n",
    "# Remove empty strings that sometimes show up\n",
    "while (corpus1.count('') > 0): \n",
    "    corpus1.remove('')\n",
    "    \n",
    "print('The number of words in the corpus is ', len(corpus1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing is done.  Now get the unique words, and encode them.\n",
    "words = sorted(list(set(corpus1)))\n",
    "num_words = len(words)\n",
    "\n",
    "# Create an encoding where each unique word in the corpus is assigned to an integer,\n",
    "# allowing us to respresent sentences as sequences of numbers\n",
    "encoding = {w: i for i, w in enumerate(words)}\n",
    "decoding = {i: w for i, w in enumerate(words)}\n",
    "\n",
    "print('We have', num_words, 'unique words.')\n",
    "corpus = corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_length=[15]\n",
    "n_LSTMs=[128]\n",
    "drop_rates=[0.1]\n",
    "\n",
    "#s_length = [5,10,50]\n",
    "#n_LSTMs=[8,32,128]\n",
    "#drop_rates=[0.0,0.5,0.8]\n",
    "\n",
    "val_accuracies=np.zeros((3,3,3,50))\n",
    "\n",
    "for sentence_indx in range(len(s_length)):\n",
    "    sentence_length = s_length[sentence_indx]\n",
    "    \n",
    "    for lstm_indx in range(len(n_LSTMs)):\n",
    "        n_LSTM = n_LSTMs[lstm_indx]\n",
    "        \n",
    "        for d_indx in range(len(drop_rates)):\n",
    "            drop_rate=drop_rates[d_indx]      \n",
    "            \n",
    "            \n",
    "            # Initialize empty lists to store the data\n",
    "            x_data = []\n",
    "            y_data = []\n",
    "\n",
    "            # Loop over the corpus, take each 50 words sequence, encode it, and save it in x_data\n",
    "            # Take each 51st word, encode it, and save it to y_data \n",
    "            for i in range(0, len(corpus) - sentence_length):\n",
    "                sentence = corpus[i: i + sentence_length]\n",
    "                next_word = corpus[i + sentence_length]\n",
    "                x_data.append([encoding[word] for word in sentence])\n",
    "                y_data.append(encoding[next_word])\n",
    "\n",
    "            # Determine the numebr of 50 word sequences (\"sentences\") in the data\n",
    "            num_sentences = len(x_data)\n",
    "            print('We have', len(x_data), 'sentences.')\n",
    "\n",
    "            # Create the variables to hold the data as it will be used.\n",
    "            x = np.zeros((num_sentences, sentence_length, num_words), dtype = np.bool)\n",
    "            y = np.zeros((num_sentences, num_words), dtype = np.bool)\n",
    "\n",
    "            # Populate the sentences. It is encoded as :\n",
    "            #                                                Index 1. Sentence Number\n",
    "            #                                                Index 2. each word in the sentence one hot encoded in the unique word list\n",
    "            print('Encoding data.')\n",
    "            for i, sentence in enumerate(x_data):\n",
    "                for t, encoded_word in enumerate(sentence):\n",
    "                    x[i, t, encoded_word] = 1\n",
    "                y[i, y_data[i]] = 1\n",
    "\n",
    "\n",
    "            print('Building network.')\n",
    "            model = km.Sequential()\n",
    "            model.add(kl.Bidirectional(kl.LSTM(n_LSTM, return_sequences=False), input_shape = (sentence_length, num_words)))# try using a bidirectional LSTM (pass data forwards and baackwards through network, since words in a sentence depend both of previous and future words)\n",
    "            model.add(kl.Dropout(drop_rate))\n",
    "            model.add(kl.Dense(num_words, activation = 'softmax'))\n",
    "            model.compile(loss = 'categorical_crossentropy', optimizer = 'RMSprop', metrics = ['accuracy'],)\n",
    "            fit = model.fit(x, y, epochs = 50, batch_size = 128,validation_split=.1,verbose=2)\n",
    "\n",
    "            val_accuracies[sentence_indx,lstm_indx,d_indx]=fit.history.get(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(x, y, epochs = 50, batch_size = 128,validation_split=.1,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose 50 words from the dictionary of words as our\n",
    "# starting sentence.\n",
    "seed = []\n",
    "for i in range(sentence_length):\n",
    "    seed.append(decoding[np.random.randint(0, num_words - 1)])\n",
    "\n",
    "# Encode the seed sentence.\n",
    "ax = np.zeros((1, sentence_length, num_words), dtype = np.bool)\n",
    "for i, w in enumerate(seed):\n",
    "    ax[0, i, encoding[w]] = 1\n",
    "\n",
    "text = ''\n",
    "\n",
    "# Run the seed sentence through the model.  Add the output to the\n",
    "# generated text.  Take the output and append it to the seed sentence\n",
    "# and remove the first word from the seed sentence.  Then repeat until\n",
    "# you've generated as many words as you like.\n",
    "for i in range(150):\n",
    "\n",
    "    # Get the most-probably next word.\n",
    "    pred = np.argmax(model.predict(ax, verbose = 0))\n",
    "\n",
    "    if i%10 == 0:\n",
    "        text+='\\n'\n",
    "    \n",
    "    # Put in verse and chorus flags for style\n",
    "    if i == 0:\n",
    "        text+=\"\\n[Verse]\\n\\n\"\n",
    "        \n",
    "    if i == 75:\n",
    "        text+=\"\\n\\n[Chorus]\\n\"\n",
    "        \n",
    "    # Add it to the generated text.\n",
    "    text += decoding[pred].capitalize()+\" \"\n",
    "    \n",
    "\n",
    "    # Encode the next word.\n",
    "    next_word = np.zeros((1, 1, num_words), dtype = np.bool)\n",
    "    next_word[0, 0, pred] = 1\n",
    "\n",
    "    # Concatenate the next word to the seed sentence, but leave off\n",
    "    # the first element so that the length stays the same.\n",
    "    ax = np.concatenate((ax[:, 1:, :], next_word), axis = 1)\n",
    "\n",
    "    \n",
    "# Print out the generated text.\n",
    "print(\"Lyrics: \\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_length = [5,10,50]\n",
    "n_LSTMs=[8,32,128]\n",
    "drop_rates=[0.0,0.5,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "figure(figsize=(20,10))\n",
    "counter=0\n",
    "for i in range(2):#(len(val_accuracies)):\n",
    "    for j in range(len(val_accuracies[i])):\n",
    "        subplot(2,3,counter+1)\n",
    "        plot(arange(0,50),ones(50)*.3,color = 'red')\n",
    "        xlabel(\"s_length=\"+str(s_length[i])+\", n_LSTM=\"+str(n_LSTMs[j]))\n",
    "        ylim(.1,.48)\n",
    "        counter+=1\n",
    "        for k in range(len(val_accuracies[i][j])):\n",
    "            if max(val_accuracies[i][j][k])!=0:\n",
    "                if drop_rates[k] == 0.8: c = \"black\"\n",
    "                if drop_rates[k] == 0.5: c = \"blue\"\n",
    "                if drop_rates[k] == 0.0: c= \"green\"\n",
    "                plot(val_accuracies[i][j][k],color=c,label=\"dropout_rate=\"+str(drop_rates[k]))\n",
    "                \n",
    "        legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
